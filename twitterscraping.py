# -*- coding: utf-8 -*-
"""twitterscraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VFEyR05vUOg-FWSdDGp7RoKrZ_1WukWy
"""

# Importing necessary libraries
import snscrape.modules.twitter as sntwitter
import pandas as pd
from datetime import datetime, timedelta
import pymongo
from pymongo import MongoClient
from datetime import date
import streamlit as st

# Connecting to MongoDB database and collection
client = pymongo.MongoClient('mongodb+srv://anasuya:helleyne123@cluster0.cpuftgg.mongodb.net/?retryWrites=true&w=majority')
db = client.twitter_scraping
records = db.tweets

# Scraping Twitter Data
def scrape_tweets(keyword, since_date, until_date, max_tweets):
    # Create an empty list to hold the scraped tweets
    tweets_list = []

    # Iterating through the tweets returned by TwitterSearchScraper object
    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f"{keyword} since:{since_date} until:{until_date} lang:en").get_items()):
        if i >= max_tweets:
            break
        
        # Creating a dictionary and extracting relevant information from the tweet
        tweet_dict = {}
        tweet_dict['date'] = tweet.date.strftime('%Y-%m-%d %H:%M:%S')
        tweet_dict['id'] = tweet.id
        tweet_dict['url'] = tweet.url
        tweet_dict['content'] = tweet.content
        tweet_dict['user'] = tweet.user.username
        tweet_dict['reply_count'] = tweet.replyCount
        tweet_dict['retweet_count'] = tweet.retweetCount
        tweet_dict['language'] = tweet.lang
        tweet_dict['source'] = tweet.sourceLabel
        tweet_dict['like_count'] = tweet.likeCount
        
        # Appending the information to tweets_list
        tweets_list.append(tweet_dict)
    
    # Storing data to MongoDB
    records.insert_one({
        'scraped_word': keyword,
        'scraped_date': datetime.utcnow(),
        'scraped_data': tweets_list
    })
    df = pd.DataFrame(tweets_list)
    return df


# Streamlit App and Graphical User Interface (GUI)
def main():
    st.title('Twitter Scraping')
    records.delete_many({})

    st.write('Enter a keyword or hashtag, date range, and number of tweets to scrape')
    keyword = st.text_input('Enter a keyword or hashtag')
    start_date = st.date_input('Start date')
    end_date = st.date_input('End date')
    max_tweets = st.number_input('Maximum number of tweets to scrape', min_value=0, max_value=1000, step=10)

    # Setting up Buttons to initiate scraping and storing process
    if st.button('Scrape Tweets'):

        # Calling the scrape_tweets function
        df = scrape_tweets(keyword, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), max_tweets)
        
        # Displaying the no. of tweets scraped successfully
        st.write(f'Successfully scraped {len(df)} tweets!')
        
        # Displaying the scraped tweets
        st.write(df)

        # Uploading data to MongoDB
        if st.button('Upload to MongoDB'):
            records.insert_many(df.to_dict('records'))
            st.write('Successfully uploaded data to MongoDB!')
        
        # Downloading data in CSV format
        if st.button('Download CSV'):
            csv = df.to_csv(index=False)
            st.download_button(label='Download CSV', data=csv, file_name=f'{keyword}_tweets.csv', mime='text/csv')
        
        # Downloading data in JSON format
        if st.button('Download JSON'):
            json = df.to_json(orient='records')
            st.download_button(label='Download JSON', data=json, file_name=f'{keyword}_tweets.json', mime='application/json')

main()